<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Vighnesh Subramaniam</title>

    <meta name="author" content="Vighnesh Subramaniam">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Vighnesh Subramaniam
                </p>
                <p>I am a first-year MIT EECS PhD student at <a href="https://www.csail.mit.edu/">CSAIL</a> advised by <a href="https://people.csail.mit.edu/boris/boris.html">Dr. Boris Katz</a> and <a href="http://0xab.com/">Dr. Andrei Barbu</a> in the MIT <a href="https://groups.csail.mit.edu/infolab/">Infolab</a>. 
                </p>
                <p>
                  Before my PhD, I obtained my bachelors (2023) and MEng (2024) in Computer Science at MIT. During this time, I was a research assistant in the Infolab specifically working on problems in deep learning, multimodal processing, and computational neuroscience. During my MEng, I also had the opportunity to work on research with <a href="https://people.csail.mit.edu/lishuang/">Dr. Shuang Li</a>, <a href="https://yilundu.github.io/">Dr. Yilun Du</a>, and <a href="https://groups.csail.mit.edu/vision/torralbalab/">Prof. Antonio Torralba</a> on problems related to generative modeling. I am extremely fortunate to be supported by the <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship</a> as well as the Robert J Shillman (1974) Fund Fellowship.
                </p>
                <p style="text-align:center">
                  <a href="mailto:vsub851@mit.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=Or3MAdgAAAAJ&view_op=list_works&sortby=pubdate">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/vsubramaniam851/">Github</a> &nbsp;/&nbsp;
                  <a href="https://x.com/su1001v">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/new_pfp.jpg" title="Photo Credits: Christopher Wang"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/new_pfp.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests mainly include multimodal deep learning, particularly NLP/computer vision, and neural network interpretability, with a touch of neuroscience/cognitive science. I highlight some publications below -- see my scholar for a more complete list.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <style>
            .research-container {
              width: 100%;
              max-width: 1000px;  /* Adjust the maximum width as needed */
              margin: auto;
              font-family: Arial, sans-serif;
            }
            .research-image {
              width: 100%;
              height: auto;
            }
          </style>

        <tr>
          <td style="padding:20px; width:40%; vertical-align:middle;">  <!-- Adjusted width for better image display -->
            <img src='images/overview_pt2.png' class="research-image" alt="Brain Treebank (Coming soon!)">
          </td>
          <td style="padding:20px; width:60%; vertical-align:middle;">  <!-- Adjusted width for more text space -->
            <a href="">
              <strong><span style="color: red;">**NEW**</span> Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli</strong>
            </a>
            <br>
            <a href="https://czlwang.github.io/">Christopher Wang*</a>,
            <a href="https://www.linkedin.com/in/adam-yaari-b0192ab4">Adam Uri Yaari*</a>,
            <a href="https://scholar.google.com/citations?user=9OPKqmMAAAAJ&hl=en">Aaditya K Singh</a>,
            <strong>Vighnesh Subramaniam</strong>,
            <a href="https://www.linkedin.com/in/dana-rosenfarb/">Dana Rosenfarb</a>,
            <a>Jan DeWitt</a>,
            <a href="https://biophysics.fas.harvard.edu/people/pranav-misra">Pranav Misra</a>,
            <a>Joseph R. Madsen</a>,
            <a>Scellig Stone</a>,
            <a href="https://klab.tch.harvard.edu/#sthash.h6RcunbQ.dpbs">Gabriel Kreiman</a>,
            <a href="https://people.csail.mit.edu/boris/boris.html">Boris Katz</a>,
            <a href="https://stanford.edu/~cases/">Ignacio Cases</a>,
            <a href="http://0xab.com/">Andrei Barbu</a>
            <br>
            <em>Neural Information Processing Systems Datasets and Benchmarks Track, 2024<span style="color: red;"> (Oral, Top 1%)</span></em>
            <br>
            <a href="https://braintreebank.dev/">Dataset</a> /
            <a href="">arXiv</a> 
            <p>The Brain Treebank is finally released! This is a large-scale dataset of intracranial recordings while subjects watch movies. We collected recordings from 10 subjects across 43 total hours. In total, subjects heard 36,000 sentences (205,000 words), while they had on average 167 electrodes implanted. </p>
          </td>
        </tr>
          
        <tr>
          <td style="padding:20px; width:40%; vertical-align:middle;">  <!-- Adjusted width for better image display -->
            <img src='images/new_overview.png' class="research-image" alt="Vision-Language Integration Diagram">
          </td>
          <td style="padding:20px; width:60%; vertical-align:middle;">  <!-- Adjusted width for more text space -->
            <a href="https://arxiv.org/abs/2406.14481">
              <strong>Revealing Vision-Language Integration in the Brain with Multimodal Networks</strong>
            </a>
            <br>
            <strong>Vighnesh Subramaniam</strong>,
            <a href="https://colinconwell.github.io/">Colin Conwell</a>,
            <a href="https://czlwang.github.io/">Christopher Wang</a>,
            <a href="https://klab.tch.harvard.edu/#sthash.h6RcunbQ.dpbs">Gabriel Kreiman</a>,
            <a href="https://people.csail.mit.edu/boris/boris.html">Boris Katz</a>,
            <a href="https://stanford.edu/~cases/">Ignacio Cases</a>,
            <a href="http://0xab.com/">Andrei Barbu</a>
            <br>
            <em>International Conference on Machine Learning, 2024</em>
            <br>
            <em>International Conference on Learning Representations: Workshop on Multimodal Representation Learning, 2023</em>
            <br>
            <a href="https://neuro-vis-lang.github.io/">Project Page</a> /
            <a href="https://arxiv.org/abs/2406.14481">arXiv</a> /
            <a href="https://github.com/vsubramaniam851/brain-multimodal">Code</a>
            <p>Multimodal deep networks of vision and language are used to localize sites of vision-language integration in the brain and identify architectural motifs that are most similar to computations in the brain.</p>
          </td>
        </tr>
          

    <tr>
      <td style="padding:20px; width:40%; vertical-align:middle;">  <!-- Adjusted width for better image display -->
          <img src="images/brainbert_overview.png" class="research-image" alt="BrainBERT Diagram">
      </td>
      <td style="padding:20px; width:60%; vertical-align:middle;">  <!-- Adjusted width for more text space -->
        <a href="https://arxiv.org/abs/2302.14367">
          <span class="papertitle">BrainBERT: Self-supervised representation learning for intracranial recordings</span>
        </a>
        <br>
        <a href="https://czlwang.github.io/">Christopher Wang</a>,
        <strong>Vighnesh Subramaniam</strong>,
        <a href="https://www.linkedin.com/in/adam-yaari-b0192ab4">Adam Uri Yaari</a>,
        <a href="https://klab.tch.harvard.edu/#sthash.h6RcunbQ.dpbs">Gabriel Kreiman</a>,
        <a href="https://people.csail.mit.edu/boris/boris.html">Boris Katz</a>,
        <a href="https://stanford.edu/~cases/">Ignacio Cases</a>,
        <a href="http://0xab.com/">Andrei Barbu</a>
        <br>
        <em>International Conference on Representation Learning</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2302.14367">arXiv</a>
        /
        <a href="https://github.com/czlwang/BrainBERT">Code</a>
        <p></p>
        <p>
        BrainBERT is a transformer-based model that learns representations on intracranial recordings for improved linear decodability from the brain.
        </p>
      </td>
    </tr>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Website design credits to <a href="https://github.com/jonbarron/jonbarron_website">John Barron</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
